{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd7ebbc-f2c8-4d36-ba08-d54e62f3b8a7",
   "metadata": {},
   "source": [
    "# Part 3: Deep Learning Model with Transformers\n",
    "## Author: Brady Lamson\n",
    "## Date: Fall 2023\n",
    "# Overview and Motivation\n",
    "\n",
    "This portion of the project has a couple distinct goals. \n",
    "\n",
    "Firstly, I shall utilize the `distilibert-base-uncased` model to attempt to predict the primary and secondary types of pokemon entirely from their text descriptions.\n",
    "\n",
    "Secondly, this notebook will function as a detailed walkthrough to fine tuning a huggingface transformer. Much of the information for this task is scattered throughout documentation and articles of varying levels of utility, so compiling all of that information into one notebook will result in something that will hopefully be useful to me and anyone who may read this. \n",
    "\n",
    "This walkthrough will also utilize hyperparameter search using the `optuna` library, which many articles I have read online seem to lack. I hope this will give this walkthrough a useful niche that other guides have not filled.\n",
    "\n",
    "## References\n",
    "\n",
    "As I've never done multi-label classification using `transformers` before I'll be using [this guide by Ronak Patel](https://colab.research.google.com/github/rap12391/transformers_multilabel_toxic/blob/master/toxic_multilabel.ipynb#scrollTo=CQQ7CoOag_r7) that is featured in [towardsdatascience](https://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1). I won't be following it 1:1 but it's there to help me get some traction.\n",
    "\n",
    "## Potential Limitations\n",
    "\n",
    "Performance of this model will be sought after but is not the end goal. I fear that the dataset I am working with will put a cap on performance. Pokemon types are extremely varied, with 19 types existing in this dataset alone. On top of that, I am predicting on both primary and secondary types which turns this into a multi-label prediction problem. Thus, combinations of types become important and many combinations of types only appear once. This is a limitation that likely cannot be overcome without removing problematic rows from the training split or simply acquiring more data. \n",
    "\n",
    "A future improvement that is outside the scope of this project is to collect all of the pokemon descriptions from each game. There are many pokemon games, and using this would allow us to duplicate many pokemon and artifically make certain type combinations more frequent and inflate our dataset. This would also provide more descriptions to train on as they tend to be similar but not identical in every game. This is obviously not without its downsides as it would inflate the frequency of already frequent type combinations, but a variant of this plan with a bit more thought put into it may be worth considering if maximizing model performance is a priority."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ebd30e-c3ea-4c89-aaa5-38cddea9675d",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "Our goal here is to do the same pre-processing as in part 2. So we'll have a bit of a repeat of that content.\n",
    "From there we'll need to convert our dataset to the transformers `DatasetDict` which will contain all of our splits. The big difference here is taking our same dataframe and doing what we need to do to it to get it working within the transforers framework.\n",
    "\n",
    "Important note that here we do NOT remove stopwords from our text as BERT in particular benefits from the full context of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c52aaa0e-5e4d-4d19-ba46-9ae133281996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(776)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4379d62f-b519-4ce1-8343-a8c70dfdcc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_name</th>\n",
       "      <th>primary_type</th>\n",
       "      <th>secondary_type</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>grass</td>\n",
       "      <td>poison</td>\n",
       "      <td>There is a plant seed on its back right from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>grass</td>\n",
       "      <td>poison</td>\n",
       "      <td>When the bulb on its back grows large, it appe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Venusaur</td>\n",
       "      <td>grass</td>\n",
       "      <td>poison</td>\n",
       "      <td>Its plant blooms when it is absorbing solar en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Charmander</td>\n",
       "      <td>fire</td>\n",
       "      <td>none</td>\n",
       "      <td>It has a preference for hot things. When it ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Charmeleon</td>\n",
       "      <td>fire</td>\n",
       "      <td>none</td>\n",
       "      <td>It has a barbaric nature. In battle, it whips ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english_name primary_type secondary_type  \\\n",
       "0    Bulbasaur        grass         poison   \n",
       "1      Ivysaur        grass         poison   \n",
       "2     Venusaur        grass         poison   \n",
       "3   Charmander         fire           none   \n",
       "4   Charmeleon         fire           none   \n",
       "\n",
       "                                         description  \n",
       "0  There is a plant seed on its back right from t...  \n",
       "1  When the bulb on its back grows large, it appe...  \n",
       "2  Its plant blooms when it is absorbing solar en...  \n",
       "3  It has a preference for hot things. When it ra...  \n",
       "4  It has a barbaric nature. In battle, it whips ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 898 entries, 0 to 897\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   english_name    898 non-null    object  \n",
      " 1   primary_type    898 non-null    category\n",
      " 2   secondary_type  898 non-null    category\n",
      " 3   description     898 non-null    object  \n",
      "dtypes: category(2), object(2)\n",
      "memory usage: 17.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_name</th>\n",
       "      <th>primary_type</th>\n",
       "      <th>secondary_type</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>898</td>\n",
       "      <td>898</td>\n",
       "      <td>898</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>898</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>water</td>\n",
       "      <td>none</td>\n",
       "      <td>Although itâ€™s alien to this world and a danger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>429</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       english_name primary_type secondary_type  \\\n",
       "count           898          898            898   \n",
       "unique          898           18             19   \n",
       "top       Bulbasaur        water           none   \n",
       "freq              1          123            429   \n",
       "\n",
       "                                              description  \n",
       "count                                                 898  \n",
       "unique                                                896  \n",
       "top     Although itâ€™s alien to this world and a danger...  \n",
       "freq                                                    3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"./data/pokemon.csv\"\n",
    "data_exists = os.path.isfile(data_path)\n",
    "\n",
    "if not data_exists:\n",
    "    # This part requires a kaggle api key. On linux this will be saved to your home directory in .kaggle/kaggle.json\n",
    "    !kaggle datasets download -d cristobalmitchell/pokedex\n",
    "    !unzip pokedex.zip -d data\n",
    "\n",
    "df = (\n",
    "    # load in the data\n",
    "    pd.read_csv(data_path, sep='\\t', encoding='utf-16-le')\n",
    "    # select the relevant columns\n",
    "    .loc[:, ['english_name', 'primary_type', 'secondary_type', 'description']]\n",
    "    # Change the type columns into categories and handle NaNs in secondary typing\n",
    "    .assign(\n",
    "        primary_type=lambda x: x['primary_type'].astype(\"category\"),\n",
    "        secondary_type=lambda x: x['secondary_type'].fillna(\"none\").astype(\"category\")\n",
    "    )\n",
    ")\n",
    "display(df.head())\n",
    "display(df.info())\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf9c68b-7ffd-4ede-8386-9fd7e0d70d01",
   "metadata": {},
   "source": [
    "## Vectorize Categorical Data\n",
    "\n",
    "Here we'll do the same one-hot encoding as in part 2. Here we'll do it before the splits though as, in retrospect, doing this after the split made no sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec216d7a-5085-42c2-810d-c5fef30c68cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_name</th>\n",
       "      <th>primary_type</th>\n",
       "      <th>secondary_type</th>\n",
       "      <th>description</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>grass</td>\n",
       "      <td>poison</td>\n",
       "      <td>There is a plant seed on its back right from t...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>grass</td>\n",
       "      <td>poison</td>\n",
       "      <td>When the bulb on its back grows large, it appe...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Venusaur</td>\n",
       "      <td>grass</td>\n",
       "      <td>poison</td>\n",
       "      <td>Its plant blooms when it is absorbing solar en...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Charmander</td>\n",
       "      <td>fire</td>\n",
       "      <td>none</td>\n",
       "      <td>It has a preference for hot things. When it ra...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Charmeleon</td>\n",
       "      <td>fire</td>\n",
       "      <td>none</td>\n",
       "      <td>It has a barbaric nature. In battle, it whips ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>Regieleki</td>\n",
       "      <td>electric</td>\n",
       "      <td>none</td>\n",
       "      <td>This PokÃ©mon is a cluster of electrical energy...</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>Regidrago</td>\n",
       "      <td>dragon</td>\n",
       "      <td>none</td>\n",
       "      <td>An academic theory proposes that Regidragoâ€™s a...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>Glastrier</td>\n",
       "      <td>ice</td>\n",
       "      <td>none</td>\n",
       "      <td>Glastrier emits intense cold from its hooves. ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>Spectrier</td>\n",
       "      <td>ghost</td>\n",
       "      <td>none</td>\n",
       "      <td>It probes its surroundings with all its senses...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>Calyrex</td>\n",
       "      <td>psychic</td>\n",
       "      <td>grass</td>\n",
       "      <td>Calyrex is a merciful PokÃ©mon, capable of prov...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>898 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    english_name primary_type secondary_type  \\\n",
       "0      Bulbasaur        grass         poison   \n",
       "1        Ivysaur        grass         poison   \n",
       "2       Venusaur        grass         poison   \n",
       "3     Charmander         fire           none   \n",
       "4     Charmeleon         fire           none   \n",
       "..           ...          ...            ...   \n",
       "893    Regieleki     electric           none   \n",
       "894    Regidrago       dragon           none   \n",
       "895    Glastrier          ice           none   \n",
       "896    Spectrier        ghost           none   \n",
       "897      Calyrex      psychic          grass   \n",
       "\n",
       "                                           description  \\\n",
       "0    There is a plant seed on its back right from t...   \n",
       "1    When the bulb on its back grows large, it appe...   \n",
       "2    Its plant blooms when it is absorbing solar en...   \n",
       "3    It has a preference for hot things. When it ra...   \n",
       "4    It has a barbaric nature. In battle, it whips ...   \n",
       "..                                                 ...   \n",
       "893  This PokÃ©mon is a cluster of electrical energy...   \n",
       "894  An academic theory proposes that Regidragoâ€™s a...   \n",
       "895  Glastrier emits intense cold from its hooves. ...   \n",
       "896  It probes its surroundings with all its senses...   \n",
       "897  Calyrex is a merciful PokÃ©mon, capable of prov...   \n",
       "\n",
       "                                                labels  \n",
       "0    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "1    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "2    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "3    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "4    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "..                                                 ...  \n",
       "893  [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "894  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "895  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "896  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "897  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...  \n",
       "\n",
       "[898 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = list(df.secondary_type.unique())\n",
    "id2label = {i: label for i, label in enumerate(names)}\n",
    "label2id = {label: i for i, label in enumerate(names)}\n",
    "\n",
    "def map_types(row):\n",
    "    type_encoding = [0] * len(names)\n",
    "    primary_id = label2id[row['primary_type']]\n",
    "    secondary_id = label2id[row['secondary_type']]\n",
    "\n",
    "    type_encoding[primary_id] = 1\n",
    "    type_encoding[secondary_id] = 1\n",
    "    \n",
    "    # return [primary_id, secondary_id]\n",
    "    return type_encoding\n",
    "\n",
    "df['labels'] = df.apply(lambda row: map_types(row), axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37635fd7-62cf-4b23-a7b9-d0bd576d8a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grass\n",
      "poison\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0][\"primary_type\"])\n",
    "print(df.iloc[0][\"secondary_type\"])\n",
    "print(df.iloc[0][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a80a81-5857-4217-a3a2-1dda7d634141",
   "metadata": {},
   "source": [
    "## Sanity Check: Verify our Preprocessing and Mapping Worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0e7b7ce-072f-4f1f-b5e9-5a5c990947ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: grass, poison\n",
      "Mapped: poison, grass\n",
      "\n",
      "Actual: grass, poison\n",
      "Mapped: poison, grass\n",
      "\n",
      "Actual: grass, poison\n",
      "Mapped: poison, grass\n",
      "\n",
      "Actual: fire, none\n",
      "Mapped: none, fire\n",
      "\n",
      "Actual: fire, none\n",
      "Mapped: none, fire\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.head().iterrows():\n",
    "    actual_primary = row[\"primary_type\"]\n",
    "    actual_secondary = row[\"secondary_type\"]\n",
    "    mapped_types = [i for i, x in enumerate(row[\"labels\"]) if x == 1]\n",
    "    mapped_primary = id2label[mapped_types[0]]\n",
    "    mapped_secondary = id2label[mapped_types[1]]\n",
    "\n",
    "    print(f\"Actual: {actual_primary}, {actual_secondary}\")\n",
    "    print(f\"Mapped: {mapped_primary}, {mapped_secondary}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b8afd8-bc90-4ee4-84db-9def570fd444",
   "metadata": {},
   "source": [
    "## Split The Dataset and Create DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5924e5a0-6070-4428-b80a-c0c70bb96f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This provides a 70/15/15 split\n",
    "train, test = train_test_split(df, test_size=0.3, random_state=10)\n",
    "test, val = train_test_split(test, test_size=0.5, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154fbf1-dd54-4fb3-bb16-1d4c3dd348a4",
   "metadata": {},
   "source": [
    "Now what we want to do is convert this into a format transformers can work with, the `Dataset` object. It's just another way of storing data is all, nothing scary. We use their `Dataset.from_pandas()` method to easily convert and provide some additional information. Really the only important part here is specifying that the labels are a \"Sequence\" of \"ClassLabels\". Or, in laymens terms, a list of class ids.\n",
    "\n",
    "Then we just use a larger container `DatasetDict` to easily store all 3 of our splits. So we don't have to juggle 5000 million different objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d971ef5c-f38a-49cc-965f-339603e2f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import (\n",
    "    Dataset, DatasetDict, Features,\n",
    "    ClassLabel, Value, Sequence\n",
    ")\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e3d19ba-bc0b-46be-94c5-9f91896e47f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(split: pd.DataFrame, mapper: dict) -> Dataset:\n",
    "    \"\"\"\n",
    "    Converts a pandas dataframe into a Dataset object\n",
    "    keeps only a handful of the columns I care about\n",
    "    \"\"\"\n",
    "    \n",
    "    names = list(mapper.keys())\n",
    "    ds = Dataset.from_pandas(\n",
    "        df = split[['english_name', 'description', 'labels']],\n",
    "        features = Features({\n",
    "            \"english_name\": Value(dtype=\"string\"),\n",
    "            \"description\": Value(dtype=\"string\"),\n",
    "            'labels': Sequence(\n",
    "                feature=Value(dtype=\"float32\"),\n",
    "                length=len(names)\n",
    "            ),\n",
    "            \"__index_level_0__\": Value(dtype=\"int64\")\n",
    "        })\n",
    "    )\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab3eb65-af9a-4b4c-a429-03e49b57233b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['english_name', 'description', 'labels', '__index_level_0__'],\n",
       "        num_rows: 628\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['english_name', 'description', 'labels', '__index_level_0__'],\n",
       "        num_rows: 135\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['english_name', 'description', 'labels', '__index_level_0__'],\n",
       "        num_rows: 135\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = DatasetDict({\n",
    "    \"train\": create_dataset(train, label2id),\n",
    "    \"test\": create_dataset(test, label2id),\n",
    "    \"val\": create_dataset(val, label2id)\n",
    "})\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c94de003-ee30-4cec-aef5-9fc6de2d418d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english_name': 'Wartortle',\n",
       " 'description': 'It is recognized as a symbol of longevity. If its shell has algae on it, that Wartortle is very old.',\n",
       " 'labels': [0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " '__index_level_0__': 7}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exampe row\n",
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9863167-7390-438c-8178-ca495ce460a9",
   "metadata": {},
   "source": [
    "## Tokenize Pokedex Description\n",
    "\n",
    "And now we use a pretrained tokenizer using the same name as what our model will use later. This tokenizer is extremely convenient and allows us to focus on other work. Though we'll want to check it's work to make sure it isn't doing any bizarre.\n",
    "\n",
    "The `Dataset` object comes with a convenient `map` method for applying functions across all its splits. \n",
    "\n",
    "The tokenizer adds 2 new columns, `input_ids` and `attention_mask`. The former is simply the tokenized version of the provided text, the latter specifies where special tokens are to allow the model to properly ignore them. A tokenier may or may not provide columns beyond this as well depending on the downstream task being used, so keep an eye out for how your dataset changes and be sure it changes how it should! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1649eb69-4768-4e4d-9806-0c067fa79d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75d7e2bf6e3495ba0bced867f3a9a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212dc640ce28429b9c061f58ba84a2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a30b4f6ff14bcba47b67d74c17c9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_description(example):\n",
    "    tokenized_desc = tokenizer(example['description'], padding='max_length', is_split_into_words=False, max_length=60)\n",
    "\n",
    "    return tokenized_desc\n",
    "\n",
    "ds = ds.map(\n",
    "    lambda example: tokenize_description(example)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35fe7c2a-6a9e-4c1e-8c09-72de95822b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['english_name', 'description', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 628\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take note of the new features\n",
    "ds[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce84cfe4-1846-438b-a7b6-564307c14588",
   "metadata": {},
   "source": [
    "### Verify tokenizer worked as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98715f31-3593-4e8a-80be-a96f98f1e79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is recognized as a symbol of longevity. if its shell has algae on it, that wartortle is very old.\n",
      "It is recognized as a symbol of longevity. If its shell has algae on it, that Wartortle is very old.\n"
     ]
    }
   ],
   "source": [
    "first_row = ds[\"train\"][0]\n",
    "print(tokenizer.decode(first_row[\"input_ids\"], skip_special_tokens=True))\n",
    "print(first_row[\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c09f4b-1268-4d32-b665-3eb10f3c8130",
   "metadata": {},
   "source": [
    "### Verify equality of lengths\n",
    "\n",
    "This is a quick visual sanity check I like to do. I want to make sure everything has been padded correctly because inequalities here can cause issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23512b70-aefd-4d72-8cf2-4b85416f70d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs Length: 60\n",
      "Attention Mask Length: 60\n",
      "\n",
      "Input IDs Length: 60\n",
      "Attention Mask Length: 60\n",
      "\n",
      "Input IDs Length: 60\n",
      "Attention Mask Length: 60\n",
      "\n",
      "Input IDs Length: 60\n",
      "Attention Mask Length: 60\n",
      "\n",
      "Input IDs Length: 60\n",
      "Attention Mask Length: 60\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, example in enumerate(ds['train']):\n",
    "    if index < 5:\n",
    "        input_ids_length = len(example['input_ids'])\n",
    "        attention_mask_length = len(example['attention_mask'])\n",
    "        \n",
    "        # Print the lengths for each feature in this row\n",
    "        print(f'Input IDs Length: {input_ids_length}')\n",
    "        print(f'Attention Mask Length: {attention_mask_length}\\n')\n",
    "\n",
    "lengths = [len(row[\"input_ids\"]) for row in ds[\"train\"]]\n",
    "max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f80db095-c497-4d4a-817d-af2dedae4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eda8b00-9c05-4b18-8516-03fc33a09948",
   "metadata": {},
   "source": [
    "# Building the Trainer\n",
    "\n",
    "The key to fine tuning with the transformers library is understanding that it all revolves around the `Trainer` object. All of what we're building here will all eventually plug into it. \n",
    "\n",
    "We need the following\n",
    "\n",
    "- A `compute_metrics()` function for evaluating performance\n",
    "- A `TrainingArguments` object\n",
    "- An `init_model()` function for use with hyperparameter search\n",
    "- A hyperparameter space object for use with, uh, also hyperparameter search\n",
    "- The `Trainer` object itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75c9b947-188e-426c-8f02-7d2bd9273ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ad744-fb2e-48ba-85fe-c20c6e7527d0",
   "metadata": {},
   "source": [
    "## Metric Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad3a48a-b689-4a41-9ac3-58c1921e7be9",
   "metadata": {},
   "source": [
    "Below is code modified from [this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb). I had to tweak stuff like changing sigmoid to softmax and adding in my own custom function for extracting the top 2 predictions. Many examples utilize a threshold but due to the large number of classes I feel like that's not a good fit for this model. \n",
    "\n",
    "Gonna be honest here, mostly taking these metrics at face-value. Stuff gets weird in multi-label models and I had a lot of trouble figuring this out on my own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "71638b2b-ba89-4b05-a8fa-b53a64c70af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from torch import tensor, topk\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply softmax to our logits to get our probabilities\n",
    "    probs = softmax(torch.Tensor(predictions), dim=0)\n",
    "\n",
    "    def tensor_to_indices(tensor):\n",
    "        # Here this function takes a tensor of probabilities and returns the indices of the two highest probs\n",
    "        _, indices = topk(tensor, 2)\n",
    "        indices = indices.tolist()\n",
    "    \n",
    "        return sorted(indices)\n",
    "    \n",
    "    # next, use said indices to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    pred_indices = tensor_to_indices(probs)\n",
    "    for index, row in enumerate(y_pred):\n",
    "        row[pred_indices[index]] = 1\n",
    "    \n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9c836-a153-4936-bec9-2152e4fc7edb",
   "metadata": {},
   "source": [
    "## Model Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e4ae34b-a854-42da-a63c-c2447cdddf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=len(names), \n",
    "        problem_type=\"multi_label_classification\", \n",
    "        id2label=id2label, \n",
    "        label2id=label2id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb7a01d3-804c-425b-bb3f-0889acdafd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_labels = len(label2id.keys())\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     model_name, num_labels=len(names), problem_type=\"multi_label_classification\", id2label=id2label, label2id=label2id\n",
    "# )\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdae808-c3a5-4d03-bc50-bf71b6eb4bc0",
   "metadata": {},
   "source": [
    "## Training Arguments and Trainer\n",
    "\n",
    "Here we set up our training arguments for hyperparameter search specifically. There'll be some slight differences to how we finally train the model. The big difference here is that I won't be saving checkpoint models for the search. This drastically increases training time and, as this is for class, it isn't worth the effort. Saving checkpoint models for each and every trial also nukes my poor hard drive as I have to save countless 500MB+ models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64a6877b-e486-4270-a6ad-2203e173dee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy='epoch',\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_strategy=\"no\",\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "cols_to_train_on = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=None,\n",
    "    model_init=model_init,\n",
    "    args=training_arguments,\n",
    "    train_dataset=ds[\"train\"].select_columns(cols_to_train_on),\n",
    "    eval_dataset=ds[\"val\"].select_columns(cols_to_train_on),\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f0c6e0-996f-4ba3-93f6-10121f2f08bf",
   "metadata": {},
   "source": [
    "# Hyperparameter Search\n",
    "\n",
    "What's neat about the `Trainer` is that it has a built in hyperparmeter search method we can leverage. To do so though we need to use a supported 3rd party library `optuna` to build a **hyperparameter space**. This is just a simple dictionary that specifies what parameters we'll be tweaking and how. \n",
    "\n",
    "## Hyperparamter Values\n",
    "\n",
    "As for hyperparameter values, I'll be leaning on stackexchange's user Mon for recommendedations and using [this thread they contributed to](https://datascience.stackexchange.com/questions/64583/what-are-the-good-parameter-ranges-for-bert-hyperparameters-while-finetuning-it) as a guide.\n",
    "\n",
    "### Epochs\n",
    "\n",
    "For LLMs smaller amounts of epochs are recommended and so I'll be using less than 20 of these. This will also reduce training time and allow me to get this project out the door faster.\n",
    "\n",
    "### Batch Size\n",
    "\n",
    "I'll be using values less than 32 here, but this will require some tinkering as values too large can sometimes devour VRAM and kill the process. This in particular will depend on the size of the tokenized text and the power of the computer used for training. The values we go with here will both need to work and also be the best performing values. \n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "We'll be sticking with values hovering around $2e^{-5}$ here as values \"larger\" can start to make the model forget too much of its original weights/biases and damage its performance. \n",
    "\n",
    "## Hyperparameter Space\n",
    "\n",
    "Now we build the object and pass in the appropriate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7412e42-f42e-48c2-a88f-c8d3f9ec31c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 2e-5, 5e-5, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "        \"per_device_eval_batch_size\": trial.suggest_categorical(\"per_device_eval_batch_size\", [8, 16, 32])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc0ab38-f028-4db7-bf06-00eafd7469b7",
   "metadata": {},
   "source": [
    "## Hyperparameter Search\n",
    "\n",
    "Small note, `direction` here will depend on what you choose for the training argument `metric_for_best_model`. I'm using the loss so we want to minimize that. This will change if you want to maximize fscore as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "febff1ca-f370-401d-a1cd-303a13120c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 14:20:41,827] A new study created in memory with name: no-name-41bac3cb-ff8a-4191-9413-7a98d218822a\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 02:11, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.528500</td>\n",
       "      <td>0.408230</td>\n",
       "      <td>0.296846</td>\n",
       "      <td>0.607323</td>\n",
       "      <td>0.022222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.369300</td>\n",
       "      <td>0.333789</td>\n",
       "      <td>0.293135</td>\n",
       "      <td>0.605246</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.323000</td>\n",
       "      <td>0.312780</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.308287</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.310200</td>\n",
       "      <td>0.307590</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.308500</td>\n",
       "      <td>0.307546</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.306200</td>\n",
       "      <td>0.306434</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.307200</td>\n",
       "      <td>0.305267</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>0.303760</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.303412</td>\n",
       "      <td>0.296846</td>\n",
       "      <td>0.607323</td>\n",
       "      <td>0.044444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 14:22:54,877] Trial 0 finished with value: 0.948613033714325 and parameters: {'learning_rate': 4.4674455649180644e-05, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 16}. Best is trial 0 with value: 0.948613033714325.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 02:10, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.577300</td>\n",
       "      <td>0.468150</td>\n",
       "      <td>0.270872</td>\n",
       "      <td>0.592787</td>\n",
       "      <td>0.029630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.424700</td>\n",
       "      <td>0.383600</td>\n",
       "      <td>0.293135</td>\n",
       "      <td>0.605246</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.365500</td>\n",
       "      <td>0.344305</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.337400</td>\n",
       "      <td>0.325920</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.324600</td>\n",
       "      <td>0.317556</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.317100</td>\n",
       "      <td>0.313639</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.312700</td>\n",
       "      <td>0.311492</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.313200</td>\n",
       "      <td>0.310451</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.310200</td>\n",
       "      <td>0.309786</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.311100</td>\n",
       "      <td>0.309695</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 14:25:07,294] Trial 1 finished with value: 0.9618075223552018 and parameters: {'learning_rate': 2.6548446161766417e-05, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32}. Best is trial 0 with value: 0.948613033714325.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 02:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.584000</td>\n",
       "      <td>0.476805</td>\n",
       "      <td>0.263451</td>\n",
       "      <td>0.588634</td>\n",
       "      <td>0.014815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.433100</td>\n",
       "      <td>0.391754</td>\n",
       "      <td>0.296846</td>\n",
       "      <td>0.607323</td>\n",
       "      <td>0.007407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.373400</td>\n",
       "      <td>0.351240</td>\n",
       "      <td>0.296846</td>\n",
       "      <td>0.607323</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.343400</td>\n",
       "      <td>0.330803</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.328900</td>\n",
       "      <td>0.320986</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>0.316007</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.313289</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.311928</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.311700</td>\n",
       "      <td>0.311093</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.312600</td>\n",
       "      <td>0.310938</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 14:27:21,109] Trial 2 finished with value: 0.9618075223552018 and parameters: {'learning_rate': 2.4649810021998845e-05, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 8}. Best is trial 0 with value: 0.948613033714325.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 02:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.600500</td>\n",
       "      <td>0.499678</td>\n",
       "      <td>0.267161</td>\n",
       "      <td>0.590711</td>\n",
       "      <td>0.014815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.454800</td>\n",
       "      <td>0.412481</td>\n",
       "      <td>0.289425</td>\n",
       "      <td>0.603170</td>\n",
       "      <td>0.022222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.393200</td>\n",
       "      <td>0.369325</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.007407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.345708</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.342700</td>\n",
       "      <td>0.332743</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.331200</td>\n",
       "      <td>0.325303</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.324000</td>\n",
       "      <td>0.320867</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.322400</td>\n",
       "      <td>0.318396</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.318300</td>\n",
       "      <td>0.316981</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.318600</td>\n",
       "      <td>0.316629</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 14:29:34,997] Trial 3 finished with value: 0.9618075223552018 and parameters: {'learning_rate': 2.0351238507399576e-05, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 8}. Best is trial 0 with value: 0.948613033714325.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 02:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.346742</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.014815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.323200</td>\n",
       "      <td>0.310571</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.309500</td>\n",
       "      <td>0.307342</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.308800</td>\n",
       "      <td>0.304680</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.302500</td>\n",
       "      <td>0.300017</td>\n",
       "      <td>0.296846</td>\n",
       "      <td>0.607323</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.292100</td>\n",
       "      <td>0.293121</td>\n",
       "      <td>0.296846</td>\n",
       "      <td>0.607323</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.276400</td>\n",
       "      <td>0.287479</td>\n",
       "      <td>0.300557</td>\n",
       "      <td>0.609399</td>\n",
       "      <td>0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.262900</td>\n",
       "      <td>0.283659</td>\n",
       "      <td>0.296846</td>\n",
       "      <td>0.607323</td>\n",
       "      <td>0.029630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.251700</td>\n",
       "      <td>0.280379</td>\n",
       "      <td>0.289425</td>\n",
       "      <td>0.603170</td>\n",
       "      <td>0.044444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.243300</td>\n",
       "      <td>0.280257</td>\n",
       "      <td>0.296846</td>\n",
       "      <td>0.607323</td>\n",
       "      <td>0.044444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-03 14:32:18,801] Trial 4 finished with value: 0.948613033714325 and parameters: {'learning_rate': 3.8659649840422755e-05, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 32}. Best is trial 0 with value: 0.948613033714325.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='0', objective=0.948613033714325, hyperparameters={'learning_rate': 4.4674455649180644e-05, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 16}, run_summary=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"minimize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=5\n",
    ")\n",
    "\n",
    "best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa14a17-892f-4ea0-953b-3d42d27c3bbb",
   "metadata": {},
   "source": [
    "So if you take the time to look at this output you may notice something bad (and hilarious), all of these models converge to the same crappy point. They all suck in the same way. This search did NOTHING. It doesn't seem like more epochs would help either, it converges in accuracy by epoch 4 on some of these. So uh, a deeper look into what parameters to change or even the metric calculation is in order. There is something fundamentally bad about how I've approached this problem.\n",
    "\n",
    "## Extract Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5aa74e03-f8dc-4f81-85ae-47e36897e810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 4.4674455649180644e-05,\n",
       " 'per_device_train_batch_size': 32,\n",
       " 'per_device_eval_batch_size': 16}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = best_trial.hyperparameters\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec57bf-a546-4acf-bb60-54eb8c67739c",
   "metadata": {},
   "source": [
    "## Clear GPU Cache\n",
    "\n",
    "There can be a lot of residual GPU memory taken up after this process, so we clear out the cache after we get the values we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ae02583-8898-4896-9e1b-be57d1936ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda import empty_cache\n",
    "\n",
    "if device == 'cuda':\n",
    "    empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51910e84-3aa6-459b-9119-1d6d554f8962",
   "metadata": {},
   "source": [
    "# Training With Best Parameters\n",
    "\n",
    "Now we set stuff up proper taking note of all the differences now. \n",
    "\n",
    "We do setup saving and set a max for the number of models that can be saved. I also setup an early stopping procedure to help prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "490ede12-8f5a-4b00-9f3b-ded4bc8c96c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    per_device_train_batch_size=best_params['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=best_params['per_device_eval_batch_size'],\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.1,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=7,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "cols_to_train_on = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=None,\n",
    "    model_init=model_init,\n",
    "    args=training_arguments,\n",
    "    train_dataset=ds[\"train\"].select_columns(cols_to_train_on),\n",
    "    eval_dataset=ds[\"val\"].select_columns(cols_to_train_on),\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d1288464-74a1-4cd4-89c1-70d114eb43c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 02:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.528500</td>\n",
       "      <td>0.408272</td>\n",
       "      <td>0.115028</td>\n",
       "      <td>0.505574</td>\n",
       "      <td>0.007407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.369300</td>\n",
       "      <td>0.333810</td>\n",
       "      <td>0.115028</td>\n",
       "      <td>0.505574</td>\n",
       "      <td>0.007407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.323000</td>\n",
       "      <td>0.312832</td>\n",
       "      <td>0.111317</td>\n",
       "      <td>0.503497</td>\n",
       "      <td>0.007407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.308321</td>\n",
       "      <td>0.126160</td>\n",
       "      <td>0.511803</td>\n",
       "      <td>0.014815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.310200</td>\n",
       "      <td>0.307632</td>\n",
       "      <td>0.137291</td>\n",
       "      <td>0.518033</td>\n",
       "      <td>0.022222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.308500</td>\n",
       "      <td>0.307586</td>\n",
       "      <td>0.115028</td>\n",
       "      <td>0.505574</td>\n",
       "      <td>0.007407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.306200</td>\n",
       "      <td>0.306489</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.509727</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.307200</td>\n",
       "      <td>0.305316</td>\n",
       "      <td>0.100186</td>\n",
       "      <td>0.497268</td>\n",
       "      <td>0.007407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>0.303793</td>\n",
       "      <td>0.111317</td>\n",
       "      <td>0.503497</td>\n",
       "      <td>0.014815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.303436</td>\n",
       "      <td>0.118738</td>\n",
       "      <td>0.507650</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.33709766149520876, metrics={'train_runtime': 144.8372, 'train_samples_per_second': 43.359, 'train_steps_per_second': 1.381, 'total_flos': 97517281636800.0, 'train_loss': 0.33709766149520876, 'epoch': 10.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fcff89-4814-483b-bb42-35dd123dc26b",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "## Examining Training Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5722a9a5-4269-42d5-aaa1-55866649da65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5285</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3693</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3230</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3121</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3102</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.3085</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>6.0</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.3062</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>7.0</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3072</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>8.0</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.3030</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.0</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.3029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss  learning_rate  epoch  step\n",
       "0  0.5285       0.000040    1.0    20\n",
       "1  0.3693       0.000036    2.0    40\n",
       "2  0.3230       0.000031    3.0    60\n",
       "3  0.3121       0.000027    4.0    80\n",
       "4  0.3102       0.000022    5.0   100\n",
       "5  0.3085       0.000018    6.0   120\n",
       "6  0.3062       0.000013    7.0   140\n",
       "7  0.3072       0.000009    8.0   160\n",
       "8  0.3030       0.000004    9.0   180\n",
       "9  0.3029       0.000000   10.0   200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.408272</td>\n",
       "      <td>0.115028</td>\n",
       "      <td>0.505574</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.8514</td>\n",
       "      <td>158.566</td>\n",
       "      <td>10.571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333810</td>\n",
       "      <td>0.115028</td>\n",
       "      <td>0.505574</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.8470</td>\n",
       "      <td>159.388</td>\n",
       "      <td>10.626</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.312832</td>\n",
       "      <td>0.111317</td>\n",
       "      <td>0.503497</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.8528</td>\n",
       "      <td>158.310</td>\n",
       "      <td>10.554</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.308321</td>\n",
       "      <td>0.126160</td>\n",
       "      <td>0.511803</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.8441</td>\n",
       "      <td>159.931</td>\n",
       "      <td>10.662</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.307632</td>\n",
       "      <td>0.137291</td>\n",
       "      <td>0.518033</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.8597</td>\n",
       "      <td>157.026</td>\n",
       "      <td>10.468</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.307586</td>\n",
       "      <td>0.115028</td>\n",
       "      <td>0.505574</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.8554</td>\n",
       "      <td>157.814</td>\n",
       "      <td>10.521</td>\n",
       "      <td>6.0</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.306489</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.509727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8561</td>\n",
       "      <td>157.697</td>\n",
       "      <td>10.513</td>\n",
       "      <td>7.0</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.305316</td>\n",
       "      <td>0.100186</td>\n",
       "      <td>0.497268</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.8594</td>\n",
       "      <td>157.088</td>\n",
       "      <td>10.473</td>\n",
       "      <td>8.0</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.303793</td>\n",
       "      <td>0.111317</td>\n",
       "      <td>0.503497</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.8539</td>\n",
       "      <td>158.100</td>\n",
       "      <td>10.540</td>\n",
       "      <td>9.0</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.303436</td>\n",
       "      <td>0.118738</td>\n",
       "      <td>0.507650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8512</td>\n",
       "      <td>158.594</td>\n",
       "      <td>10.573</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eval_loss   eval_f1  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "0   0.408272  0.115028      0.505574       0.007407        0.8514   \n",
       "1   0.333810  0.115028      0.505574       0.007407        0.8470   \n",
       "2   0.312832  0.111317      0.503497       0.007407        0.8528   \n",
       "3   0.308321  0.126160      0.511803       0.014815        0.8441   \n",
       "4   0.307632  0.137291      0.518033       0.022222        0.8597   \n",
       "5   0.307586  0.115028      0.505574       0.007407        0.8554   \n",
       "6   0.306489  0.122449      0.509727       0.000000        0.8561   \n",
       "7   0.305316  0.100186      0.497268       0.007407        0.8594   \n",
       "8   0.303793  0.111317      0.503497       0.014815        0.8539   \n",
       "9   0.303436  0.118738      0.507650       0.000000        0.8512   \n",
       "\n",
       "   eval_samples_per_second  eval_steps_per_second  epoch  step  \n",
       "0                  158.566                 10.571    1.0    20  \n",
       "1                  159.388                 10.626    2.0    40  \n",
       "2                  158.310                 10.554    3.0    60  \n",
       "3                  159.931                 10.662    4.0    80  \n",
       "4                  157.026                 10.468    5.0   100  \n",
       "5                  157.814                 10.521    6.0   120  \n",
       "6                  157.697                 10.513    7.0   140  \n",
       "7                  157.088                 10.473    8.0   160  \n",
       "8                  158.100                 10.540    9.0   180  \n",
       "9                  158.594                 10.573   10.0   200  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "train_logs = []\n",
    "valid_logs = []\n",
    "for index, x in enumerate(trainer.state.log_history):\n",
    "    if 'loss' in x.keys():\n",
    "        train_logs.append(x)\n",
    "    elif 'eval_loss' in x.keys():\n",
    "        valid_logs.append(x)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "train_logs = pd.DataFrame(train_logs)\n",
    "valid_logs = pd.DataFrame(valid_logs)\n",
    "\n",
    "display(train_logs)\n",
    "display(valid_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1aa5ea7a-0813-499a-91d7-d8eaa2d7c91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(valid_logs.epoch, valid_logs.eval_f1, color='red', label='F1 Score')\n",
    "plt.plot(valid_logs.epoch, valid_logs.eval_accuracy, color='orange', label='Accuracy')\n",
    "plt.plot(valid_logs.epoch, valid_logs.eval_roc_auc, color='purple', label='ROC AUC')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Validation Metrics\")\n",
    "plt.xticks(np.arange(0, 11, 2))\n",
    "plt.yticks(np.arange(0,1.1,.1))\n",
    "plt.legend()\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a0d42-51be-449b-95e8-3927019c1cfc",
   "metadata": {},
   "source": [
    "This is perhaps the funniest fine tuning plot I have ever seen in my life. This model sucks SO BAD. This is the funniest thing I have ever seen in my entire life I have never created something so hell-bent on refusing to improve. This is fantastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abff94ba-4067-4c42-91b5-46ee45213e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_results = trainer.predict(ds[\"test\"].select_columns(cols_to_train_on))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28082007-82e7-4450-baa7-5b1098ec0a9e",
   "metadata": {},
   "source": [
    "## Testing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65b06c05-f34f-4f5f-add0-b0afc86c45d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-2.3712327 , -0.31074724, -1.793147  , ..., -2.6443713 ,\n",
       "        -2.2080014 , -1.9563926 ],\n",
       "       [-2.2860775 , -0.24917479, -1.9477478 , ..., -2.8598552 ,\n",
       "        -2.1890223 , -2.0565896 ],\n",
       "       [-2.313064  , -0.15123682, -2.1181304 , ..., -2.8251402 ,\n",
       "        -2.2538269 , -2.1162133 ],\n",
       "       ...,\n",
       "       [-2.3708131 , -0.26421148, -1.9273932 , ..., -2.7729497 ,\n",
       "        -2.2259245 , -2.0452256 ],\n",
       "       [-2.355137  , -0.2656626 , -1.8036344 , ..., -2.736625  ,\n",
       "        -2.1617198 , -1.9295585 ],\n",
       "       [-2.362205  , -0.26113003, -1.9145846 , ..., -2.83908   ,\n",
       "        -2.2564363 , -2.0735292 ]], dtype=float32), label_ids=array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 1.]], dtype=float32), metrics={'test_loss': 0.28956642746925354, 'test_f1': 0.37407407407407406, 'test_roc_auc': 0.6502178649237472, 'test_accuracy': 0.08888888888888889, 'test_runtime': 0.98, 'test_samples_per_second': 137.754, 'test_steps_per_second': 9.184})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79845516-e42c-4bd9-bebc-04e88d7d6839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.28956642746925354,\n",
       " 'test_f1': 0.37407407407407406,\n",
       " 'test_roc_auc': 0.6502178649237472,\n",
       " 'test_accuracy': 0.08888888888888889,\n",
       " 'test_runtime': 0.98,\n",
       " 'test_samples_per_second': 137.754,\n",
       " 'test_steps_per_second': 9.184}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41868376-a562-4b3c-ba72-7e95191cd1ac",
   "metadata": {},
   "source": [
    "# Show Predictions (they're bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "760a33ed-f7f4-4d72-8780-2792424f4f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pokemon: Electabuzz\n",
      "True: ['none', 'electric']\n",
      "Predicted: ['none', 'flying']\n",
      "---\n",
      "Pokemon: Scolipede\n",
      "True: ['poison', 'bug']\n",
      "Predicted: ['none', 'water']\n",
      "---\n",
      "Pokemon: Cosmog\n",
      "True: ['none', 'psychic']\n",
      "Predicted: ['none', 'water']\n",
      "---\n",
      "Pokemon: Melmetal\n",
      "True: ['none', 'steel']\n",
      "Predicted: ['none', 'water']\n",
      "---\n",
      "Pokemon: Klefki\n",
      "True: ['fairy', 'steel']\n",
      "Predicted: ['none', 'flying']\n",
      "---\n",
      "Pokemon: Ducklett\n",
      "True: ['flying', 'water']\n",
      "Predicted: ['none', 'water']\n",
      "---\n",
      "Pokemon: Articuno\n",
      "True: ['flying', 'ice']\n",
      "Predicted: ['none', 'water']\n",
      "---\n",
      "Pokemon: Sandygast\n",
      "True: ['ground', 'ghost']\n",
      "Predicted: ['none', 'water']\n",
      "---\n",
      "Pokemon: Tyrogue\n",
      "True: ['none', 'fighting']\n",
      "Predicted: ['none', 'water']\n",
      "---\n",
      "Pokemon: Rookidee\n",
      "True: ['none', 'flying']\n",
      "Predicted: ['none', 'flying']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for index, logit in enumerate(test_results.predictions):\n",
    "    if index < 10:\n",
    "    \n",
    "        probs = softmax(torch.Tensor(logit), dim=0)\n",
    "    \n",
    "        def tensor_to_indices(tensor):\n",
    "            # Here this function takes a tensor of probabilities and returns the indices of the two highest probs\n",
    "            _, indices = topk(tensor, 2)\n",
    "            indices = indices.tolist()\n",
    "        \n",
    "            return sorted(indices)\n",
    "    \n",
    "        # Show pokemon name\n",
    "        name = ds[\"test\"][\"english_name\"][index]\n",
    "        print(f\"Pokemon: {name}\")\n",
    "    \n",
    "        # convert true labels to types\n",
    "        true_types = ds[\"test\"][\"labels\"][index]\n",
    "        true_types = tensor_to_indices(true_types)\n",
    "        true_types = [id2label[index] for index in true_types]\n",
    "        print(f\"True: {true_types}\")\n",
    "        \n",
    "        # Convert predictions to types\n",
    "        pred_indices = tensor_to_indices(probs)\n",
    "        pred_types = [id2label[index] for index in pred_indices]\n",
    "        print(f\"Predicted: {pred_types}\")\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb728d-a20f-4d8b-af6d-27617138c7de",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "It doesn't take a pokemon master to note that this model is terrible. It has similar issues as the part 2 model actually, it constantly predicts types we knew would be common. \"None\", \"Water\" and \"Flying\". It basically never deviates from this. Almost every single prediction contains none and one of those two types. I only show 10 of the predictions here but trust me, this behavior continues. \n",
    "\n",
    "I am baffled by how poorly this model performs and how little things changed during fine tuning. I believe I am missing something fundamental to how this type of problem should be tackled. I've never done multi label classification and it's very possible that I butchered the entire process. Is it how I handled the `None` types? Is it how I handled the labeling? The metric computataions are a likely suspect as well as that was code I took and modified and as such don't understand very well.\n",
    "\n",
    "Oh well! I've never failed this horrendously on building a model before so it was very entertaining to see how things look when a model totally sucks. This was actually very insightful. I'm going to have to come back to this at a later date. \n",
    "\n",
    "Also I should have shown how to set up a pipeline and such but I don't believe it's necessary. I've gotten what I wanted out of this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
